{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, n_heads, embd_dim, in_proj_bias=True, out_proj_bias=True):\n",
    "    super().__init__()\n",
    "    self.n_heads = n_heads\n",
    "    self.in_proj = nn.Linear(embd_dim, 3 * embd_dim, bias=in_proj_bias)\n",
    "    self.out_proj = nn.Linear(embd_dim, embd_dim, bias=out_proj_bias)\n",
    "\n",
    "    self.d_heads = embd_dim // n_heads\n",
    "\n",
    "  def forward(self, x, casual_mask=False):\n",
    "    # x: (batch_size, seq_len, dim)\n",
    "\n",
    "    batch_size, seq_len, d_emed = x.shape\n",
    "\n",
    "    interim_shape = (batch_size, seq_len, self.n_heads, self.d_heads)\n",
    "\n",
    "    # (batch_size, seq_len, dim) -> 3 * (batch_size, seq_len, d_embed)\n",
    "    q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "    # change the shape of q, k and v to match the interim shape\n",
    "    q = q.view(interim_shape)\n",
    "    k = k.view(interim_shape)\n",
    "    v = v.view(interim_shape)\n",
    "\n",
    "    # swap the elements within matrix using transpose\n",
    "    # take n_heads before seq_len, like that: (batch_size, n_heads, seq_len, d_embed)\n",
    "    q = q.transpose(1, 2)\n",
    "    k = k.transpose(1, 2)\n",
    "    v = v.transpose(1, 2)\n",
    "\n",
    "    # calculate the attention\n",
    "    weight = q @ k.transpose(-1, -2)\n",
    "\n",
    "    if casual_mask:\n",
    "        # mask where the upper traingle (above the prinicpal dagonal) is 1\n",
    "        mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
    "        # fill the upper traingle with -inf\n",
    "        weight.masked_fill_(mask, -torch.inf)\n",
    "\n",
    "    weight /= math.sqrt(self.d_heads)\n",
    "\n",
    "    weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "    # (batch_size, h_heads, seq_len, dim / h)\n",
    "    output = weight @ v\n",
    "\n",
    "    # (batch_size, h_heads, seq_len, dim / h) -> (batch_size, seq_len, n_heads, dim / h)\n",
    "    output = output.transpose(1, 2)\n",
    "\n",
    "    # change the shape to the shape of out_proj\n",
    "    output = output.reshape((batch_size, seq_len, d_emed))\n",
    "\n",
    "    output = self.out_proj(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "      super().__init__()\n",
    "      self.groupnorm = nn.GroupNorm(32, channels)\n",
    "      self.attention = SelfAttention(1, channels)\n",
    "\n",
    "  def forward(self, x):\n",
    "      # x: (batch_size, channels, h, w)\n",
    "      residual = x.clone()\n",
    "\n",
    "      # (batch_size, channels, h, w) -> (batch_size, channels, h, w)\n",
    "      x = self.groupnorm(x)\n",
    "\n",
    "      n, c, h, w = x.shape\n",
    "\n",
    "      # (batch_size, channels, h, w) -> (batch_size, channels, h * w)\n",
    "      x = x.view((n, c, h * w))\n",
    "\n",
    "      # (batch_size, channels, h * w) -> (batch_size, h * w, channels)\n",
    "      x = x.transpose(-1, -2)\n",
    "\n",
    "      # perform self-attention without mask\n",
    "      # (batch_size, h * w, channels) -> (batch_size, h * w, channels)\n",
    "      x = self.attention(x)\n",
    "\n",
    "      # (batch_size, h * w, channels) -> (batch_size, channels, h * w)\n",
    "      x = x.transpose(-1, -2)\n",
    "\n",
    "      # (batch_size, channels, h * w) -> (batch_size, channels, h, w)\n",
    "      x = x.view((n, c, h, w))\n",
    "\n",
    "      # (batch_size, channels, h, w) -> (batch_size, channels, h, w)\n",
    "      x += residual\n",
    "\n",
    "      return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super().__init__()\n",
    "    self.groupnorm1 = nn.GroupNorm(32, in_channels)\n",
    "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    self.groupnorm2 = nn.GroupNorm(32, out_channels)\n",
    "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    if in_channels == out_channels:\n",
    "      self.residual_layer = nn.Identity()\n",
    "    else:\n",
    "      self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x: (batch_size, in_channels, h, w)\n",
    "    residue = x.clone()\n",
    "\n",
    "    x = self.groupnorm1(x)\n",
    "    x = F.selu(x)\n",
    "    x = self.conv1(x)\n",
    "    x = self.groupnorm2(x)\n",
    "    x = self.conv2(x)\n",
    "\n",
    "    return x + self.residual_layer(residue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Sequential):\n",
    "    def  __init__(self):\n",
    "        super().__init__(\n",
    "            # (batch_size, channel, h, w) -> (batch_size, 128, h, w)\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "\n",
    "            # (batch_size, 128, h, w) -> (batch_size, 128, h, w)\n",
    "            ResidualBlock(128, 128),\n",
    "\n",
    "            # (batch_size, 128, h, w) -> (batch_size, 128, h / 2, w / 2)\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # (batch_size, 128, h / 2, w / 2) -> (batch_size, 256, h / 2, w / 2)\n",
    "            ResidualBlock(128, 256),\n",
    "\n",
    "            # (batch_size, 256, h / 2, w / 2) -> (batch_size, 256, h / 2, w / 2)\n",
    "            ResidualBlock(256, 256),\n",
    "\n",
    "            # (batch_size, 256, h / 2, w / 2) -> (batch_size, 256, h / 4, w / 4)\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # (batch_size, 256, h / 4, w / 4) -> (batch_size, 512, h / 4, w / 4)\n",
    "            ResidualBlock(256, 512),\n",
    "\n",
    "            # (batch_size, 512, h / 4, w / 4) -> (batch_size, 512, h / 4, w / 4)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, h / 4, w / 4) -> (batch_size, 512, h / 8, w / 8)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # (batch_size, 512, h / 8, w / 8) -> (batch_size, 512, h / 8, w / 8)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, h / 8, w / 8) -> (batch_size, 512, h / 8, w / 8)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, h / 8, w / 8) -> (batch_size, 512, h / 8, w / 8)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, h / 8, w / 8) -> (batch_size, 512, h / 8, w / 8)\n",
    "            AttentionBlock(512),\n",
    "\n",
    "            # (batch_size, 512, h / 8, w / 8) -> (batch_size, 512, h / 8, w / 8)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, h / 8, w / 8) -> (batch_size, 512, h / 8, w / 8)\n",
    "            nn.GroupNorm(32, 512),\n",
    "\n",
    "            # (batch_size, 512, h / 8, w / 8) -> (batch_size, 512, h / 8, w / 8)\n",
    "            nn.SiLU(),\n",
    "\n",
    "            # (batch_size, 512, h / 8, w / 8) -> (batch_size, 8, h / 8, w / 8)\n",
    "            nn.Conv2d(512, 8, kernel_size=3, padding=1),\n",
    "\n",
    "            # (batch_size, 8, h / 8, w / 8) -> (batch_size, 8, h / 8, w / 8)\n",
    "            nn.Conv2d(8, 8, kernel_size=1, padding=0)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, channel, h, w)\n",
    "\n",
    "        for module in self:\n",
    "            if isinstance(module, nn.Conv2d) and module.stride == (2, 2):\n",
    "                x = F.pad(x, (0, 1, 0, 1))  # (left, right, top, bottom)\n",
    "            x = module(x)\n",
    "\n",
    "        # (batch_size, 8, h / 8, w / 8) -> two tensors of shape (batch_size, 4, h / 8, w / 8)\n",
    "        mean, log_variance = torch.chunk(x, 2, dim=1)\n",
    "\n",
    "        # Clamp log variance between -30 and 20\n",
    "        log_variance = torch.clamp(log_variance, -30, 20)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * log_variance)\n",
    "        eps = torch.randn_like(std)\n",
    "        x = mean + eps * std\n",
    "\n",
    "        # Scale the latent representation\n",
    "        x *= 0.18215\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            # (batch_size, 4, 32, 32) -> (batch_size, 512, 32, 32)\n",
    "            nn.Conv2d(4, 512, kernel_size=3, padding=1),\n",
    "\n",
    "            # (batch_size, 512, 32, 32) -> (batch_size, 512, 32, 32)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_Size, 512, 32, 32) -> (batch_size, 512, 32, 32)\n",
    "            AttentionBlock(512),\n",
    "\n",
    "            # (batch_size, 512, 32, 32) -> (batch_size, 512, 32, 32)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, 32, 32) -> (batch_size, 512, 32, 32)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, 32, 32) -> (batch_size, 512, 32, 32)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, 32, 32) -> (batch_size, 512, 64, 64)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "\n",
    "            # (batch_size, 512, 64, 64) -> (batch_size, 512, 64, 64)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "\n",
    "            # (batch_size, 512, 64, 64) -> (batch_size, 512, 64, 64)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, 64, 64) -> (batch_size, 512, 64, 64)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, 64, 64) -> (batch_size, 512, 64, 64)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, 64, 64) -> (batch_size, 512, 128, 128)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "\n",
    "            # (batch_size, 512, 128, 128) -> (batch_size, 512, 128, 128)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "\n",
    "            # (batch_size, 512, 128, 128) -> (batch_size, 256, 128, 128)\n",
    "            ResidualBlock(512, 256),\n",
    "\n",
    "            # (batch_size, 256, 128, 128) -> (batch_size, 256, 128, 128)\n",
    "            ResidualBlock(256, 256),\n",
    "\n",
    "            # (batch_size, 256, 128, 128) -> (batch_size, 256, 128, 128)\n",
    "            ResidualBlock(256, 256),\n",
    "\n",
    "            # (batch_size, 256, 128, 128) -> (batch_size, 256, 256, 256)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "\n",
    "            # (batch_size, 256, 256, 256) -> (batch_size, 256, 256, 256)\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "\n",
    "            # (batch_size, 256, 256, 256) -> (batch_size, 128, 256, 256)\n",
    "            ResidualBlock(256, 128),\n",
    "\n",
    "            # (batch_size, 128, 256, 256) -> (batch_size, 128, 256, 256)\n",
    "            ResidualBlock(128, 128),\n",
    "\n",
    "            # (batch_size, 128, 256, 256) -> (batch_size, 128, 256, 256)\n",
    "            ResidualBlock(128, 128),\n",
    "\n",
    "            nn.GroupNorm(32, 128),\n",
    "\n",
    "            nn.SiLU(),\n",
    "\n",
    "            # (batch_size, 128, 256, 256) -> (batch_size, 3, 256, 256)\n",
    "            nn.Conv2d(128, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 4, h / 8, w / 8)\n",
    "\n",
    "        # remove the scaling adding by the encoder\n",
    "        x /= 0.18215\n",
    "\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "\n",
    "        # (batch_size, 3, h, w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1KXRTB_q4uub_XOHecpsQjE4Kmv76sZbV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip all-dogs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(source_dir, train_dir, test_dir, test_size=0.2, random_state=42):\n",
    "    image_files = [f for f in os.listdir(source_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    train_files, test_files = train_test_split(image_files, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(train_dir, file))\n",
    "\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(test_dir, file))\n",
    "\n",
    "    print(f\"Dataset split complete. {len(train_files)} training images, {len(test_files)} test images.\")\n",
    "\n",
    "source_dir = \"./all-dogs\"\n",
    "train_dir = \"./data/train/dogs\"\n",
    "test_dir = \"./data/test/dogs\"\n",
    "\n",
    "split_dataset(source_dir, train_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "# from model import Encoder, Decoder\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "beta = 0.00025  # KL divergence weight\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((56, 56)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "batch_size = 4\n",
    "dataset = torchvision.datasets.ImageFolder(root='./data/train', transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Add these hyperparameters\n",
    "accumulation_steps = 1  # Adjust as needed\n",
    "effective_batch_size = batch_size * accumulation_steps\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed, encoded = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        recon_loss = nn.MSELoss()(reconstructed, images)\n",
    "\n",
    "        # Extract mean and log_variance from encoded\n",
    "        mean, log_variance = torch.chunk(encoded, 2, dim=1)\n",
    "\n",
    "        kl_div = -0.5 * torch.sum(1 + log_variance - mean.pow(2) - log_variance.exp())\n",
    "        loss = recon_loss + beta * kl_div\n",
    "\n",
    "        # Normalize the loss to account for accumulation\n",
    "        loss = loss / accumulation_steps\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], '\n",
    "              f'Loss: {loss.item()*accumulation_steps:.4f}, Recon Loss: {recon_loss.item():.4f}, KL Div: {kl_div.item():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Take the first image from the batch\n",
    "            sample_image = images[0].unsqueeze(0)\n",
    "            sample_reconstructed = model(sample_image)[0]\n",
    "\n",
    "            sample_image = (sample_image * 0.5) + 0.5\n",
    "            sample_reconstructed = (sample_reconstructed * 0.5) + 0.5\n",
    "\n",
    "            torchvision.utils.save_image(sample_reconstructed, 'reconstructed.png')\n",
    "\n",
    "    train_losses.append(train_loss / len(dataloader))\n",
    "  # Save the model checkpoint\n",
    "    torch.save(model.state_dict(), f'vae_model_epoch_{epoch+1}.pth')\n",
    "\n",
    "print('Training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the loss curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VAE Loss over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
